{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615687c21cd3379c",
   "metadata": {},
   "source": [
    "# Chapter 6 Linear Model Selection and Regularization\n",
    "## Exercise 1\n",
    "### 1.a\n",
    "Among the three models - best subset, forward stepwise, and backward stepwise - the model with $k$ predictors has the smallest training RSS is guaranteed to be the best subset model. This is because best subset selection performs an exhaustive search over all possible combinations of $k$ predictors to find the absolute minimal value of training RSS. Forward and backward stepwise approximate the solution by finding the locally optimal choice, which may not lead to the globally optimal combinations of predictors.\n",
    "### 1.b\n",
    "There is not enough information to determine which model will yield the lowest test RSS. This is due to the bias-variance trade-off among these three approaches:\n",
    "* Best subset selection: due to its larger search space, best subset selection has lower bias (less constraint) while higher variance (more models to explore) than stepwise selection. In this context, high variance can lead to overfitting, resulting in low test RSS.\n",
    "* Forward and backward stepwise selection: due to its constrained search space compared to best subset selection, stepwise selection has higher bias (more constraint) while lower variance (less models to explore) than best subset collection. In this context, higher bias can hinder finding the true variable combination that best explains the relationship between predictors and response, leading to low test RSS.\n",
    "\n",
    "While stepwise methods often perform better when N is small relative to p (due to variance reduction), best subset selection may perform better when N is large or when the stepwise methods fail to capture the true signal.\n",
    "### 1.c\n",
    "* (i) True. This is because the $(k+1)$-variable model is identified by adding a new variable to the $k$-variable model in forward stepwise.\n",
    "* (ii) True. This is because the $k$-variable model is identified by removing a variable from the $(k+1)$-variable model in backward stepwise.\n",
    "* (iii) False. This is because there is no guarantee that forward stepwise and backward stepwise will yield the same k-variable model.\n",
    "* (iv) False. This is because there is no guarantee that forward stepwise and backward stepwise will yield the same k-variable model.\n",
    "* (v) False. This is because the best model of size $k$ is optimizing RSS for $k$ predictors. Similarly, the best model of size $k+1$ is optimizing RSS for $k+1$ predictors. There are no constraints forcing the $k$ model to be a subset of the $k+1$ model.\n",
    "\n",
    "## Exercise 2\n",
    "### 2.a\n",
    "Compared to least squares, the lasso adds additional constraints to the coefficient estimates, thus is generally less flexible (higher bias and lower variance) than least squares (except for $\\lambda = 0$, which is when the lasso is equivalent to least squares). So, (i) and (ii) are incorrect. Since the lasso aims to improve model prediction accuracy by reducing variance at the cost of increase in bias, it can only so when its increase in bias is less than its decrease in variance.\n",
    "So, (iii) is the correct one.\n",
    "### 2.b\n",
    "Ridge regression is similar to the lasso. So, the response and reasoning is the same as 2.a.\n",
    "### 2.c\n",
    "Generally speaking, non-linear methods are more flexible than least squares method because they can fit a wider range of data shapes. So, (iii) and (iv) are incorrect. Due to their flexibility, they generally have lower bias (they can fit the truth better) but they have higher variance (they are prone to fitting for the noise in the data). So, they can improve prediction accuracy when the decrease in bias outweighs the increase in variance. In other words, they can improve prediction accuracy if their increase in variance is less than their decrease in bias. So, (ii) is the correct one.\n",
    "\n",
    "## Exercise 3\n",
    "The regularization term has the form of the lasso: $\\sum_{j=1}^{p}{|\\beta_j|} \\leq s$.\n",
    "### 3.a\n",
    "* As $s=0$, the constraint on the model is maximum, forcing the model to have maximum bias while minimal variance. At this high level of bias, the coefficient estimates might fail to capture the relationship between the predictors and response. Thus, RSS is high.\n",
    "* As $s$ approaches $\\infty$, the effect of the regularization term is minimized, thus, the model becomes a simply least squares. The training RSS will approach the training RSS of least squares models. As the constraint enforced by the regularization term is relaxed, the model is prone to overfitting and will try to capture even the noise in the data, thus obtaining minimal training RSS.\n",
    "So, the correct answer is (iv): the training RSS will steadily decrease.\n",
    "### 3.b\n",
    "* As $s=0$, the constraint on the model is maximum, forcing the model to have maximum bias while minimal variance. Consequently, the coefficient estimates might fail to capture the relationship between the predictors and response in the data (underfitting). So, at this point, test RSS is very high.\n",
    "* As $s$ increases from 0, the constraint on the model decreases, hence, reducing the bias of the model while increasing the variance in the model. While the decrease in bias outweighs the increase in variance, the model starts to effectively capture the relationship between the predictors and response in the data, so test RSS will decrease.\n",
    "* Test RSS will reach its minimum point when the decrease in bias meets the increase in variance.\n",
    "* After this point, the increase in variance offsets the decrease in bias, negatively affecting the model fit on the data. So, test RSS will increase.\n",
    "So, the correct answer is (ii): as $s$ increases from 0, test RSS will decrease initially, and then eventually start increasing in a U shape.\n",
    "### 3.c\n",
    "As $s$ increases from 0, the constraint on the model decreases, hence, reducing the bias of the model while increasing the variance in the model. However, past a certain value of $s$, bias approaches its minimal value and variance its maximal value. So, the correct answer is (iii) with a caveat: variance steadily increases and eventually plateau.\n",
    "### 3.d\n",
    "As $s$ increases from 0, the constraint on the model decreases, hence, reducing the bias of the model while increasing the variance in the model. However, past a certain value of $s$, bias approaches its minimal value and variance its maximal value. So, the correct answer is (iv) with a caveat: bias steadily decreases and eventually plateau.\n",
    "### 3.e\n",
    "Irreducible error is the inherent noise in the data that is independent of the model. So, the regularization term doesn't affect the irreducible error. So, the correct response is (v): the irreducible error remains constant.\n",
    "\n",
    "## Exercise 4\n",
    "The regularization term has the form of ridge regression: $$\\lambda \\sum_{j=1}^{p}{\\beta_j^2}$$.\n",
    "As $\\lambda=0$, the model becomes least squares.\n",
    "### 4.a\n",
    "As $\\lambda=0$, the model becomes least squares. The training RSS will equal that of the least squares model. As $\\lambda$ increases, the regularization term adds more constraints on the model, increasing its bias while reducing its variance. For a training data, as the bias of the model increases, its ability to capture the relationship between predictors and response reduces as it becomes less flexible. So, training RSS will increase. So, the correct response is (iii): training RSS will steadily increase.\n",
    "### 4.b\n",
    "* As $\\lambda=0$, the constraint of the regularization on the model is minimal. So, the model will have low bias and high variance. Consequently, the model is more prone to overfitting to the training data. So, although RSS will be low, test RSS might be high as the model also try to fit the noise in the data.\n",
    "* As $\\lambda$ increases, the regularization term enforces more constraint on the model, resulting in its increase in bias while decrease in variance. The increase bias and decrease in variance serve shrink the magnitude of coefficients to dampening the effect of noise and reducing variance. So, the test RSS will likely to decrease as long as the benefits of the decrease in variance outweigh the increase in bias.\n",
    "* As the benefits of decreased variance is countered by the inflexibility of the model due to increased bias, the test RSS will reach its minimal point. After this point, since the induced benefits of decreased variance no longer serve to improve the model fit, the effect of bias starts to dominate, making the model increasingly inflexible to capture the true relationship between predictors and response. So, test RSS will increase.\n",
    "\n",
    "So, the correct answer is (ii): test RSS decrease initially, and then eventually start increasing in a U shape.\n",
    "### 4.c\n",
    "As $\\lambda$ increases from 0, the constraints of the regularization term becomes stronger, increasing the model bias while reducing its variance. After a certain value of $\\lambda$, the model variance approaches its minimal, while model bias its maximal. So, the correct answer is (iv) with a caveat: the model variance steadily decreases and eventually plateau.\n",
    "### 4.d\n",
    "As $\\lambda$ increases from 0, the constraints of the regularization term becomes stronger, increasing the model bias while reducing its variance. After a certain value of $\\lambda$, the model variance approaches its minimal, while model bias its maximal. So, the correct answer is (iii) with a caveat: the model bias steadily increases and eventually plateau.\n",
    "### 4.e\n",
    "Irreducible error is the inherent noise in the data that is independent of the model. So, the regularization term doesn't affect the irreducible error. So, the correct response is (v): the irreducible error remains constant.\n",
    "\n",
    "## Exercise 5\n",
    "### 5.a\n",
    "The ridge regression aims to minimize: $$\\sum_{i=1}^{n}{(y_i - \\beta_0 - \\sum_{j=1}^{p}{\\beta_jx_{ij}})^2} + \\lambda\\sum_{j=1}^{p}{\\beta_j^2}$$\n",
    "Given that $\\beta_0 = 0$, $x_{i1} = x_{i2}$, the optimization term becomes: $$\\sum_{i=1}^{n}{(y_i - x_i\\sum_{j=1}^{p}{\\beta_j})^2} + \\lambda\\sum_{j=1}^{p}{\\beta_j^2} (2)$$\n",
    "Given $n=2$ and $p=2$, the optimization term becomes: $$[y_1 - x_1(\\beta_1 + \\beta_2)]^2 + [y_2 - x_2(\\beta_1 + \\beta_2)]^2 + \\lambda(\\beta_1^2 + \\beta_2^2)$$\n",
    "### 5.b\n",
    "Since the optimization term aims to minimize RSS through $\\beta_j$, to find the minimal value of the optimization term, we take its partial differential equation in each of the $\\beta_j$ direction, and then set the partial differential equations to 0. The partial differential equations are:\n",
    "$$\\frac{d}{d\\beta_1} = -2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] + 2\\lambda\\beta_1$$\n",
    "$$\\frac{d}{d\\beta_2} = -2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] + 2\\lambda\\beta_2$$\n",
    "Let $a = -2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)]$, the 2 partial differentials become:\n",
    "$$\\frac{d}{d\\beta_1} = a + 2\\lambda\\beta_1$$\n",
    "$$\\frac{d}{d\\beta_2} = a + 2\\lambda\\beta_2$$\n",
    "Setting these 2 partial differential equations to 0, we have the following: $$a + 2\\lambda\\beta_1 = a + 2\\lambda\\beta_2 = 0$$\n",
    "$$\\leftrightarrow \\beta_1 = \\beta_2$$\n",
    "So, in this setting, the ridge coefficient estimates satisfy: $\\hat{\\beta_1} = \\hat{\\beta_2}$\n",
    "### 5.c\n",
    "The lasso optimization aims to minimize: $$\\sum_{i=1}^{n}{(y_i - \\beta_0 - \\sum_{j=1}^{p}{\\beta_jx_{ij}})^2} + \\lambda\\sum_{j=1}^{p}{|\\beta_j|}$$\n",
    "Similarly to (a), following the constraints, the optimization term becomes: $$[y_1 - x_1(\\beta_1 + \\beta_2)]^2 + [y_2 - x_2(\\beta_1 + \\beta_2)]^2 + \\lambda(|\\beta_1| + |\\beta_2|)$$\n",
    "### 5.d\n",
    "Since the optimization term aims to minimize RSS through $\\beta_j$, to find the minimal value of the optimization term, we take its partial differential equation in each of the $\\beta_j$ direction, and then set the partial differential equations to 0. The partial differential equations are:\n",
    "$$\n",
    "\\frac{d}{d\\beta_1} =\n",
    "\\begin{cases}\n",
    "-2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] - \\lambda & \\text{if } \\beta_1 < 0 \\\\\n",
    "-2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] + \\lambda & \\text{if } \\beta_1 \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "\\frac{d}{d\\beta_2} =\n",
    "\\begin{cases}\n",
    "-2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] - \\lambda & \\text{if } \\beta_2 < 0 \\\\\n",
    "-2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] + \\lambda & \\text{if } \\beta_2 \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Let $a = -2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)]$, the 2 partial differentials become:\n",
    "$$\n",
    "\\frac{d}{d\\beta_1} =\n",
    "\\begin{cases}\n",
    "a - \\lambda & \\text{if } \\beta_1 < 0 \\\\\n",
    "a + \\lambda & \\text{if } \\beta_1 \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "\\frac{d}{d\\beta_2} =\n",
    "\\begin{cases}\n",
    "a - \\lambda & \\text{if } \\beta_2 < 0 \\\\\n",
    "a + \\lambda & \\text{if } \\beta_2 \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Setting these 2 partial differential equations to 0, we have the following cases:\n",
    "* Case 1: $\\beta_1 < 0$ and $\\beta_2 \\geq 0$, then $a - \\lambda = a + \\lambda = 0 \\leftrightarrow \\lambda = 0 \\text{ and } a = 0 $. This means that the lasso optimization term is minimal, the model becomes least squares. Then, the coefficient estimates is constrained by: $$a = -2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] = 0$$ $$\\leftrightarrow \\beta_1 + \\beta_2 = \\frac{x_1y_1 + x_2y_2}{x_1^2 + x_2^2}$$\n",
    "* Case 2: $\\beta_1 \\geq 0$ and $\\beta_2 < 0$, then $a - \\lambda = a + \\lambda = 0 \\leftrightarrow \\lambda = 0 \\text{ and } a = 0 $. This means that the lasso optimization term is minimal, the model becomes least squares. Similar to case 1, the coefficient estimates is constrained by: $$\\beta_1 + \\beta_2 = \\frac{x_1y_1 + x_2y_2}{x_1^2 + x_2^2}$$\n",
    "* Case 3: $\\beta_1 < 0$ and $\\beta_2 < 0$, then the coefficient estimates are constrained by: $$a - \\lambda = 0$$ $$\\leftrightarrow a = \\lambda$$ $$\\leftrightarrow a = -2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] = \\lambda$$ $$\\leftrightarrow \\beta_1 + \\beta_2 = \\lambda + \\frac{x_1y_1 + x_2y_2}{x_1^2 + x_2^2}$$\n",
    "* Case 4: $\\beta_1 \\geq 0$ and $\\beta_2 \\geq 0$, then the coefficient estimates are constrained by: $$a + \\lambda = 0$$ $$\\leftrightarrow a = -\\lambda$$ $$\\leftrightarrow a = -2x_1[y_1 - x_1(\\beta_1 + \\beta_2)] -2x_2[y_1 - x_2(\\beta_1 + \\beta_2)] = -\\lambda$$ $$\\leftrightarrow \\beta_1 + \\beta_2 = -\\lambda + \\frac{x_1y_1 + x_2y_2}{x_1^2 + x_2^2}$$\n",
    "\n",
    "Among these 4 cases, only case 3 and case 4 are applicable to the lasso optimization since case 1 and case 2 the model is constrained to become least squares. So, in this setting, the lasso coefficients $\\hat{\\beta_1} \\text{ and } \\hat{\\beta_2}$ are not unique because the solution is any pair of $\\hat{\\beta_1} \\text{ and } \\hat{\\beta_2}$ that satisfies the constraints in case 3 and case 4:\n",
    "$$\n",
    "\\hat{\\beta_1} + \\hat{\\beta_2} =\n",
    "\\begin{cases}\n",
    "\\lambda + \\frac{x_1y_1 + x_2y_2}{x_1^2 + x_2^2} & \\text{if } \\hat{\\beta_1} < 0 \\text{ and } \\hat{\\beta_2} < 0 \\\\\n",
    "-\\lambda + \\frac{x_1y_1 + x_2y_2}{x_1^2 + x_2^2} & \\text{if } \\hat{\\beta_1} \\geq 0 \\text{ and } \\hat{\\beta_2} \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Exercise 6\n",
    "### 6.a\n",
    "$$ \\sum_{j=1}^{p} (y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2  \\text{ (6.12)} $$\n",
    "Given that p=1, the optimization problem is:\n",
    "$$ (y_1 - \\beta_1)^2 + \\lambda \\beta_1^2 $$\n",
    "\n",
    "\n",
    "$$ \\hat{\\beta}_j^R = y_j / (1 + \\lambda) \\text{ (6.14)} $$\n",
    "### 6.b\n",
    "$$ \\sum_{j=1}^{p} (y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\text{ (6.13)} $$\n",
    "$$\n",
    "\\hat{\\beta}_j^L = \\begin{cases} \n",
    "y_j - \\lambda/2 & \\text{if } y_j > \\lambda/2; \\\\\n",
    "y_j + \\lambda/2 & \\text{if } y_j < -\\lambda/2; \\\\\n",
    "0 & \\text{if } |y_j| \\leq \\lambda/2.\n",
    "\\end{cases}\n",
    "\\text{ (6.15)}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 6.a\n",
    "y1 = 2\n",
    "lambda1 = 3\n",
    "b1 = np.linspace(-3, 3, 100)\n",
    "\n",
    "# 1. create arrays\n",
    "opt_ridge = (y1 - b1)**2 + lambda1 * b1**2\n",
    "y_cases = [2, -2, 1]\n",
    "opt_lasso_arrays = [(y - b1)**2 + lambda1 * np.abs(b1) for y in y_cases]\n",
    "opt_array = np.vstack([opt_ridge] + opt_lasso_arrays)\n",
    "\n",
    "# 2. create labels\n",
    "labels = [\n",
    "    \"Ridge optimization\",\n",
    "    \"Lasso optimization (y1 > lambda/2)\",\n",
    "    \"Lasso optimization (y1 < -lambda/2)\",\n",
    "    \"Lasso optimization (|y1| <= lambda/2)\"\n",
    "]\n",
    "\n",
    "# 3. create vertical lines arguments\n",
    "x_lines = [\n",
    "    y1 / (1 + lambda1),\n",
    "    y_cases[0] - lambda1/2,  # y > lambda/2\n",
    "    y_cases[1] + lambda1/2,  # y < -lambda/2\n",
    "    0                        # |y| <= lambda/2\n",
    "]\n",
    "\n",
    "# 4. create subplots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(b1, opt_array[i])\n",
    "    plt.axvline(x=x_lines[i], color=\"r\", linestyle=\"--\", label=f\"x={x_lines[i]:.2f}\")\n",
    "    plt.title(labels[i])\n",
    "    plt.xlabel(\"b1\")\n",
    "    plt.ylabel(\"Objective\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-legacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}